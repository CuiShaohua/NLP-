{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips of using gensim Tfidf_Vec<br/>\n",
    "_________\n",
    "### steps:<br />\n",
    "* 1 docs ---> dictionary  <br/>\n",
    ". a. docs is a list consist of docutmnet <br/>\n",
    ". b. use the ```jieba.cut(doc, cut_all=False)``` segmenting list<br/>\n",
    ". c. use the gensim ```corpora.Dictionary(words)``` to construst a dictionary<br/>\n",
    ". d. dictionary has many attributes(cfs,dfs,token2id...)<br/>\n",
    ". e. use the new variable ```new_corpus= [dictionary.doc2bow(doc) for doc in docs]``` <br/>\n",
    "* 2 dictionary --> gensim Tfidf model<br/>\n",
    ".a. from gensim import models<br/>\n",
    ".b. acquire tfidf training model```models.TfidfModel(new_corpus)```<br/>\n",
    ".c. fun using the model to acquire vector of tfidf ``` tfidf_model[string_bow] ```<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import TfIdfTransformer\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [['红色', '的', '水果', '是', '什么'],\n",
    " [ '黎明', '中间', '是', '我'],\n",
    " ['我', '在', '小红', '和','黎明','之间'],\n",
    " ['是', '那里', '在', '我', '你']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = [dictionary.doc2bow(t) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(new_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ idf = add + log_{log\\_base} \\frac{total~docs}{doc~freq}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python实现TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([['a','b','b','c'],['c','d','e','b','f'],['s','a','d','e'],['a','e','s','w','c']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 1, 'b': 2, 'c': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = {'a':1,'b':2,'c':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.sum(list(Counter(ss).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_wlocal(x):\n",
    "\n",
    "    ll = dict() # term frequency\n",
    "    for i, m in Counter(x).items():\n",
    "        ll[i] = Counter(x)[i] / np.sum(list(Counter(x).values()))\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(x[0])['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.16666666666666666, 'b': 0.3333333333333333, 'c': 0.5}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_wlocal(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_wglobal(x):\n",
    "    # 遍历一遍，拿到所有的单词\n",
    "    words = set()\n",
    "    for n in x:\n",
    "        for i in n:\n",
    "            words.add(i)\n",
    "    #针对每个不重复单词尽心文档的计数\n",
    "    doc = defaultdict(float)\n",
    "    try:\n",
    "        for word in words:\n",
    "            # 查找每个文档\n",
    "            for n in x:\n",
    "                if word in n:\n",
    "                    doc[word] += 1\n",
    "            #print(doc[word])\n",
    "            doc[word] = log10(len(x) / (1.0 + doc[word]))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'f': 0.3010299956639812,\n",
       "             'e': 0.0,\n",
       "             'c': 0.0,\n",
       "             'w': 0.3010299956639812,\n",
       "             'd': 0.12493873660829993,\n",
       "             'a': 0.0,\n",
       "             'b': 0.12493873660829993,\n",
       "             's': 0.12493873660829993})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_wglobal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得TfIdf值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每篇文档的TF-IDF值d\n",
    "def avg_tfidf(x, tf, idf):\n",
    "    # 每篇文档tf\n",
    "    Tf = [tf(tf_id) for tf_id in x]\n",
    "    # 总idf\n",
    "    Idf = idf(x)\n",
    "    # 计算每篇的TF-IDF值\n",
    "    text_of_tfidf = defaultdict(float)\n",
    "    doc_of_tfidf = defaultdict(list)\n",
    "    h = 0\n",
    "    for doc in Tf:\n",
    "        for i, m in doc.items():\n",
    "            if i in Idf:\n",
    "                text_of_tfidf[i] = doc[i] * Idf[i]\n",
    "                \n",
    "        h += 1\n",
    "        doc_of_tfidf['doc'+ str(h)] = text_of_tfidf\n",
    "        text_of_tfidf = defaultdict(float)\n",
    "    return DataFrame(doc_of_tfidf).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = avg_tfidf(x, custom_wlocal, custom_wglobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>doc4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.062469</td>\n",
       "      <td>0.024988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024988</td>\n",
       "      <td>0.031235</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031235</td>\n",
       "      <td>0.024988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc1      doc2      doc3      doc4\n",
       "a  0.000000  0.000000  0.000000  0.000000\n",
       "b  0.062469  0.024988  0.000000  0.000000\n",
       "c  0.000000  0.000000  0.000000  0.000000\n",
       "d  0.000000  0.024988  0.031235  0.000000\n",
       "e  0.000000  0.000000  0.000000  0.000000\n",
       "f  0.000000  0.060206  0.000000  0.000000\n",
       "s  0.000000  0.000000  0.031235  0.024988\n",
       "w  0.000000  0.000000  0.000000  0.060206"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '2019年国家网络安全宣传周开幕式16日在天津举行。中共中央政治局委员、中宣部部长黄坤明在开幕式上宣读习近平的重要指示并讲话。他说，要认真学习贯彻习近平总书记重要指示精神，深刻把握信息化发展大势，积极应对网络安全挑战，充分发挥广大人民在维护网络安全中的主体作用，把“四个坚持”的原则要求落到实处，有力维护人民群众在网络空间的切身利益。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    regex = r\"(\\d+|\\w+)\"\n",
    "    ret = re.findall(regex,string=string)\n",
    "    words = list()\n",
    "    for p in ret:\n",
    "        words += list(jieba.cut(p, cut_all=False))\n",
    "    #[list(jieba.cut(p, cut_all=False)) for p in ret]\n",
    "    return words\n",
    "#list(jieba.cut(string, cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\C00495~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.662 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(custom_wlocal(cut(text1)).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"据介绍，第二次演练包括庆祝大会仪式、阅兵、群众游行、联欢活动、转场及应急处置6项演练内容，重点磨合内部衔接，全面检验了各方组织指挥和保障体系运行工作。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('据介绍', 0.030303030303030304),\n",
       " ('第二次', 0.030303030303030304),\n",
       " ('演练', 0.06060606060606061),\n",
       " ('包括', 0.030303030303030304),\n",
       " ('庆祝大会', 0.030303030303030304),\n",
       " ('仪式', 0.030303030303030304),\n",
       " ('阅兵', 0.030303030303030304),\n",
       " ('群众', 0.030303030303030304),\n",
       " ('游行', 0.030303030303030304),\n",
       " ('联欢', 0.030303030303030304),\n",
       " ('活动', 0.030303030303030304),\n",
       " ('转场', 0.030303030303030304),\n",
       " ('及', 0.030303030303030304),\n",
       " ('应急', 0.030303030303030304),\n",
       " ('处置', 0.030303030303030304),\n",
       " ('6', 0.030303030303030304),\n",
       " ('项', 0.030303030303030304),\n",
       " ('内容', 0.030303030303030304),\n",
       " ('重点', 0.030303030303030304),\n",
       " ('磨合', 0.030303030303030304),\n",
       " ('内部', 0.030303030303030304),\n",
       " ('衔接', 0.030303030303030304),\n",
       " ('全面', 0.030303030303030304),\n",
       " ('检验', 0.030303030303030304),\n",
       " ('了', 0.030303030303030304),\n",
       " ('各方', 0.030303030303030304),\n",
       " ('组织', 0.030303030303030304),\n",
       " ('指挥', 0.030303030303030304),\n",
       " ('和', 0.030303030303030304),\n",
       " ('保障体系', 0.030303030303030304),\n",
       " ('运行', 0.030303030303030304),\n",
       " ('工作', 0.030303030303030304)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(custom_wlocal(cut(text2)).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"近日，新疆军区某陆航旅组织部队采取空地一体的方式，机动至天山北麓某地域，展开了多机型、多弹种、多靶型跨昼夜火力突击演练。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = \"夜幕降临，飞行员驾驶直升机再次升空，接受攻击指令，采取红外瞄准的方式，发射导弹，对目标进行精准火力打击摧毁，并释放干扰弹。在持续进行了14个小时的大强度饱和攻击状态后，实弹射击演练结束\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [text1,text2, text3, text4]\n",
    "tmp_text = [cut(t) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = tmp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 4), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 4), (39, 1), (40, 2), (41, 3), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (53, 1)], [(43, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 2), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1)], [(38, 1), (55, 1), (74, 1), (77, 1), (85, 1), (86, 1), (87, 1), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 2), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1)], [(18, 1), (25, 1), (38, 2), (55, 1), (74, 1), (93, 1), (99, 1), (106, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 2), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 2), (138, 1), (139, 1), (140, 1), (141, 1)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "# 赋给语料库中每个词(不重复的词)一个整数id\n",
    "dictionary = corpora.Dictionary(word_list)\n",
    "new_corpus = [dictionary.doc2bow(text) for text in word_list]\n",
    "print(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'this is the first document',\n",
    "    'this is the second second document',\n",
    "    'and the third one',\n",
    "    'is this the first document'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in range(len(corpus)):\n",
    "    word_list.append(corpus[i].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(new_corpus)\n",
    "tfidf.save(\"my_model.tfidf\")\n",
    "\n",
    "# 载入模型\n",
    "tfidf = models.TfidfModel.load(\"my_model.tfidf\")\n",
    "\n",
    "# 使用这个训练好的模型得到单词的tfidf值\n",
    "tfidf_vec = []\n",
    "for i in range(len(corpus)):\n",
    "    string = corpus[i]\n",
    "    string_bow = dictionary.doc2bow(string.lower().split())\n",
    "    string_tfidf = tfidf[string_bow]\n",
    "    tfidf_vec.append(string_tfidf)\n",
    "print(tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = avg_tfidf(tmp_text, custom_wlocal, custom_wglobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_of_tfidf(dataframe):\n",
    "    num = dataframe.shape[1]\n",
    "    for i in range(num): \n",
    "        for j in range(1,num):\n",
    "            fenzi = np.dot(dataframe.iloc[i], dataframe.iloc[j])\n",
    "            fenmu = (sqrt(np.dot(dataframe.iloc[i],dataframe.iloc[i]))*sqrt(np.dot(dataframe.iloc[j],dataframe.iloc[j])))\n",
    "        \n",
    "        #cos_vec_tdidf = fenzi / fenmu\n",
    "            print(i, i+1,fenzi/fenmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0.0\n",
      "0 1 0.0\n",
      "0 1 0.0\n",
      "1 2 1.0\n",
      "1 2 1.0\n",
      "1 2 0.0\n",
      "2 3 1.0\n",
      "2 3 1.0\n",
      "2 3 0.0\n",
      "3 4 0.0\n",
      "3 4 0.0\n",
      "3 4 1.0\n"
     ]
    }
   ],
   "source": [
    "vec_of_tfidf(data)  # 4个文本的cos值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
