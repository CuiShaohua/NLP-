{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Can you come up out 3 sceneraies which use AI methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器翻译、语音识别、智能机器人"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How do we use Github; Why do we use Jupyter and Pycharm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub代码仓库，方便多个用户进行管理，同时可以设置别人修改代码版本，jupyter 在线编辑器，Pycharm工程编译器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用贝叶斯概率公式来处理AI问题。对NLP来讲，通过计算有短语组成的句子在数据集中出现的概率进行判断句子的合理性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Can you came up with some sceneraies at which we could use Probability Model？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Microsoft Word中的语言拼写检查；语音识别中的文字匹配；验证某些疾病的发病率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解析和模式匹配之前没有研究过，看了一些KMP算法，但是没懂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What's the Language Model？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model 中文就叫做“语言模型”吧，这实际上是一个概率分布模型 P ，对于语言里的每一个字符串 S 给出一个概率 P(S) 。稍微正式一点的定义可以这样说：假设有一个符号的集合 \\{w\\} ，我们不妨把每一个 w_i 称作一个“单词”，由零个或多个单词连接起来就组成了一个字符串 S = w_1w_2\\cdots w_n ，字符串可长可短，例如实际语言中的句子、段落或者文档都可以看作一个字符串。所有合法（例如，通过一些语法约束）的字符串的集合称作一个语言，而一个语言模型就是这个语言里的字符串的概率分布模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引自：http://blog.pluskid.org/?p=352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们是不是应该给些资料？要不然到处找别人写的博客，还不知道他说的对不对。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Can you came up with some sceneraies at which we could use Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 一篇文档，也许是通过扫描 OCR 识别得到的，或者是部分破译的密文，以及部分损坏了的文档等等，总之有一些地方字词缺失，有一些不同的可能，这个时候就可以用 Language Model 来计算并选择概率最大的那一种。\n",
    "\n",
    "2 专门建立特定的 Language Model 也是很常见的，比如，如果把程序看成是一个指令的序列的话，那么可以分别构建两个 Language Model ：\\mathcal{M}_1 和 \\mathcal{M}_2 ，分别代表正常程序和病毒的模型，给定一个程序，只要分别用两个模型计算出对应的概率，比较一下谁大谁小，就可以判断出该程序是不是病毒了。\n",
    "\n",
    "3 也有像搜索引擎那样建立很多个 Language Model 的情况，通常搜索引擎在检索时会先通过倒排表等非常高效的手段从海量的数据里找出相关的内容，然后会使用相对复杂但是更精确一些的算法来为这些结果排序（毕竟大部分用户都只会关心第一页甚至只是前几项搜索结果），抛开著名的 PageRank 不说，使用 Language Model 也是一种比较常见的手段。具体做法是每个文档会有一个对应的 Language Model ，在 rank 的时候使用这些 Language Model 来计算 query 的概率，最后按照概率值由大到小排序即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. What's the 1-gram language model？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子中的每个短语只和自己有关，因此总体的概率就是，Pr = TT(Pr(wi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What's the disadvantages and advantages of 1-gram language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：就是计算比较简单，只计算数据集中出现过每单个短语的概率是多少就可以了，然后将每个概率相乘就是句子的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺点：体现不出句子的整体概念，因为一个句子存在必然前后是有联系的，所以才会出现2_gram和3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. What't the 2-gram models？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑到句子的整体概念，前后短语之间必然存在联系，因此，2_gram考虑的是前词和邻词的关系，且只认为他们俩有关系，跨越邻词之后的第三个、第四个、第m个词都与第一个词无关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算2_gram的公式是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(w1w2w3w4) = Pr(w1|w2)*Pr(w2|w3)Pr(w3|w4)*Pr(w4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个单独2元组单独的概率计算法则是：\n",
    "$ Pr(w1|w2) = \\frac {Counter(w1 + w2)} {Counter(w2)} $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
