{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras拆解多分类的使用实例详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   sklearn.model_selection.train_test_split(*arrays, **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```arrays```分割对象同样长度的列表或者numpy arrays（矩阵）；  \n",
    "```test_size```两种指定方法。1：指定小数，小数范围在0.0-0.1之间，它代表test占数据集的比例。\n",
    "2 指定整数。整数的大小必须在数据集个数范围内，如果test_size没有指定，可以通过train_size来指定。如果train_size也没有指定，那么默认值是0.25。  \n",
    "```train_size```和test_size类似  \n",
    "```random_state```:这是将分割的training和testing 集合打乱的个数设定，如果不指定的话，也可以通过numpy.random来设定随机数。e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 读取train.csv\n",
    "df = pd.read_csv('./jigsaw-toxic-comment-classification-challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "namelist_train, namelist_test = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47872, 8), (111699, 8))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namelist_test.shape, namelist_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 8), (159471, 8))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namelist_train, namelist_test = train_test_split(df, test_size=100)\n",
    "namelist_test.shape, namelist_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```怎么评价一个模型的好坏？```  \n",
    "<b>```Regression```</b>是直接按误差，$\\frac{\\Delta t}{t}$，但是<b>```Classification```</b>则不好评估，因此有以下几个指标： \n",
    "* 1 Accuracy： 评价的所有结果中有几个正确的，不做类别细粒度区分。$Acc = \\frac{n_{评价正确的个数}}{N_{评价结果总个数}}$  \n",
    "* 2 Precision: 有更细类别的区分，预测正确的个数/针对某个类别的预测结果个数。$Precision = \\frac{t_p}{t_p+f_p}$  \n",
    "* 3 Recall召回率：有更细的类别区分。预测结果中正确的个数/原本标签中此类别的个数。$Recall = \\frac{t_p}{t_p+t_f}$\n",
    "* 4 AUC：Aera under the curving，在预测曲线ROC之下的面积，这个面积越大说明模型效果越好。AUC牵扯几个指标：  \n",
    "> True_Postive_Rate: TPR---$\\frac{TP}{TP+FN}$ 预测的正类中实际正实例站所有正实例的比例  \n",
    "> False_Postive_Rate: FPR---$\\frac {FP}{FP+TN}$ 预测的正类中实际是负实例占所有负实例的比例\n",
    "#而在划分正负类的阈值变化$0-->1$情况下，FPR为横轴，TPR为纵轴建立ROC曲线，那么在这个曲线之下的面积就是AUC，TPR=1，FPR=0,即坐标图中的(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.5 0.5 1. ] [0.  0.5 0.5 1.  1. ] [1.8  0.8  0.4  0.35 0.1 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "y_true = np.array([1,1,2,2])\n",
    "y_score = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, threshoulds = roc_curve(y_true, y_score, pos_label=2)\n",
    "print(fpr, tpr, threshoulds)\n",
    "roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "? roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 解答上述问题需要明白其中的2点，y_score是什么？y_score是指预测结果为正实例的概率。pos_label默认值是None，y_true在{-1,1}或{0,1}之内，此时pos_label=1，而在其他情况下必须指定pos_label。上述计算过程:\n",
    "threshould是从score的高到低开始，当threshould取0.8时，score大于0.8的值就是正实例，预测结果为$[0,0,0,1]$  \n",
    "那么对于正样本【2】来说，预测结果中只有一个预测对了，因此TPR计算：  \n",
    "$$TPR = \\frac{t_p}{t_p + f_n} = \\frac{1}{1+1}=0.5$$\n",
    "而对于负样本【1】来说，预测结果都是正确的，那么$f_p = 0$,因此FPR计算：\n",
    "$$FPR = \\frac{f_p}{f_p + t_n} = \\frac{0}{0+2}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当阈值取0.4时，score中大于等于0.4的均是正实例，那么此时$\\hat{y}= [1,2,1,2]$，那么正负样本中各有一个预测错了，，因此$$TPR=FPR=0.5$$  \n",
    "* 而当阈值取0.35时，对应的预测结果为$[1,2,2,2]$，那么正样本【2】中全预测正确，但负样本【1】中有一个预测正确，此时$$TPR =1,~~FPR =0.5$$  \n",
    "* 当阈值取0.1时，预测结果为$[2,2,2,2]$，那么正样本【2】结果预测全部正确，但负样本【1】预测全部不正确，因此$$TPR=1, FPR=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "roc_curve可以用下面的图来表示：\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown \n",
    "roc_curve可以用下面的图来表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x204cb038a20>,\n",
       "  <matplotlib.axis.YTick at 0x204cb038358>],\n",
       " <a list of 2 Text yticklabel objects>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "tpr = [0.5, 0.5, 1,   1]\n",
    "fpr = [0,   0.5, 0.5, 1]\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xticks([0,1])\n",
    "plt.yticks([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 因此根据折线下面的面积，$$AUC = 0.75$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 上述的计算过程实际上是将```【1】```和```【2】```看做多个类，y_score是得分，表示预测结果是```【1】```或者```【2】```的正确概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>* from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)  \n",
    "将多个序列截断或补齐为相同的长度。  \n",
    "该函数将一个 num_samples 的序列（整数列表）转化为一个 2D Numpy 矩阵，其尺寸为 (num_samples, num_timesteps)。 num_timesteps 要么是给定的 maxlen 参数，要么是最长序列的长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比```num_timesteps```长的序列将会被截断以满足所需要的长度。补齐或截断发生的位置分别由```padding```和```turncating```决定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向前补齐为默认操作。即$padding = truncating = pre$， e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 0,  9,  1, 23,  4],\n",
       "       [ 7,  8, 94,  4,  2]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg_sequences = np.array([[1,2,3,4,5],[9,1,23,4],[33,5,6,7,8,94,4,2]])\n",
    "eg_max_len = 5\n",
    "sequence.pad_sequences(eg_sequences,eg_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 9,  1, 23,  4,  0],\n",
       "       [33,  5,  6,  7,  8]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.pad_sequences(eg_sequences,eg_max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x204d227a0b8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.Tokenizer(num_words=None,\n",
    "               filters='!\"#$$%^&*\"',\n",
    "               lower=True,\n",
    "               split=' ',\n",
    "               char_level=False,\n",
    "               oov_token=None,\n",
    "               document_count=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本标记实用类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该类允许实用两种方法向量化一个文本语料库：将每个文本转化为一个整数序列（每个整数都是词典中标记的索引）；或者将其转化为一个向量，其中每个标记的系数可以是二进制值、词频、TF_IDF权重等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_words: 需要保留的最大词数，基于词频。只有最常出现的num_words词会被保留；  \n",
    "filter：一个字符串，其中每个元素是一个这个文本中过滤掉的字符，默认值是所有标点符号，加上制表符和换行符，减去```'```字符。  \n",
    "lower: 布尔值。是否将文本转换为小写。  \n",
    "split: 字符串。按该字符串切割文本。  \n",
    "char_level:如果为True，则每个字符都将被视为标记。  \n",
    "oov_token: 如果给出，他将被添加到word_index中，并用于在text_to_sequence调用期间替换词汇表外的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 默认情况下，删除所有的标点符号，将文本转换为空格分割的单词索引，但单词中可能含有```'```字符。这些序列然后被分割成标记列表。然后将被索引和向量化。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_tokennizer = text.Tokenizer(num_words=20)\n",
    "eg_tokennizer.fit_on_texts(['i am a china, do you know lily\\'s bird, would you love me?','i love huahua, dou you love me?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('i', 2),\n",
       "             ('am', 1),\n",
       "             ('a', 1),\n",
       "             ('china', 1),\n",
       "             ('do', 1),\n",
       "             ('you', 3),\n",
       "             ('know', 1),\n",
       "             (\"lily's\", 1),\n",
       "             ('bird', 1),\n",
       "             ('would', 1),\n",
       "             ('love', 3),\n",
       "             ('me', 2),\n",
       "             ('huahua', 1),\n",
       "             ('dou', 1)])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg_tokennizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 1,\n",
       " 'love': 2,\n",
       " 'i': 3,\n",
       " 'me': 4,\n",
       " 'am': 5,\n",
       " 'a': 6,\n",
       " 'china': 7,\n",
       " 'do': 8,\n",
       " 'know': 9,\n",
       " \"lily's\": 10,\n",
       " 'bird': 11,\n",
       " 'would': 12,\n",
       " 'huahua': 13,\n",
       " 'dou': 14}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg_tokennizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* text_to_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text.text_to_word_sequence(text,\n",
    "                           filters='$',\n",
    "                           lower=True,\n",
    "                           split=' '\n",
    ")\n",
    "将文本转换为单词（或标记）的序列  \n",
    "text: 输入的文本（字符串）。  \n",
    "filter：要过滤的字符列表（或连接），如标点符号，默认是基本标点符号，制表符和换行符。\n",
    "lower:布尔值，是否将文本转换为小写\n",
    "split：字符值，按该字符串切割文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = eg_tokennizer.texts_to_sequences(['i am a china, do you know lily\\'s bird, would you love me?','i love huahua, dou you love me?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 5, 6, 7, 8, 1, 9, 10, 11, 12, 1, 2, 4], [3, 2, 13, 14, 1, 2, 4]]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = sequence.pad_sequences(X_, maxlen=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 中文单词被数字化后也可以使用sequence.pad_sequence()进行截断和补齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 12,  1,  2,  4],\n",
       "       [13, 14,  1,  2,  4]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>* from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras中的model类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b > * from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)```   \n",
    "\\\n",
    "将正整数（索引值）转换为固定尺寸的稠密向量。  \n",
    "imput_dim: int>0。词汇表大小，即最大整数index+1。  \n",
    "output_dim: int>0。词向量的维度。  \n",
    "embeddings_initializer: ```embeddings```矩阵初始化方法（详见initializers）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子1  Toxic Comment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './crawl-300d-2M.vec/crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('./jigsaw-toxic-comment-classification-challenge/train.csv')\n",
    "test = pd.read_csv('./jigsaw-toxic-comment-classification-challenge/test.csv')\n",
    "submission = pd.read_csv('./jigsaw-toxic-comment-classification-challenge/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000  # 理解为3W个句子的词典\n",
    "maxlen = 100  # 单个句子的最大长度\n",
    "embed_size = 300  # 词向量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)  # 建立句子库，句子数目为30000\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))  # 填充句子库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen) # 一句话长为100单词\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')  # asarray不会copy新的副本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))  # 构建新的词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 到此为止构建词向量矩阵完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##下面开始建立AUC评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)  # AUC值\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])  # 按照axis=-1（行）串联起来输出张量\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      " - 1336s - loss: 0.0495 - accuracy: 0.9821 - val_loss: 0.0459 - val_accuracy: 0.9823\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986562 \n",
      "\n",
      "Epoch 2/2\n",
      " - 1278s - loss: 0.0375 - accuracy: 0.9854 - val_loss: 0.0447 - val_accuracy: 0.9831\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.987162 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  665,    22,     1,  5178,    18,  2276,     2,    16,   497,\n",
       "         412,    25,    60,    61,  8238,   939,    17,     8,   828,\n",
       "           1,   198,    10,  3778,   151,     4,   412,    10,     1,\n",
       "        2665,    27,  1272,     3,  1085,   763,  2215,   763,   162,\n",
       "           7,   125,     7,     8,     1, 11370,   562,  1968,     6,\n",
       "          39,    66,   350,    13,    17,   763,   162,   739,   162,\n",
       "         501,   125,   739,     8,     1,   325,   562,  1968,     5,\n",
       "         501,     8,     1,  1968,     3,  7777,    84,     6,    94,\n",
       "           1,   114,  6293,    12,   739,    11,  9160,     2,     7,\n",
       "         501,    80,   501,     8,   610,    25,    22,     1,  5178,\n",
       "          18,  6062,   163,     9,   288,    77,     4, 10604,    43,\n",
       "          34])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tra[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
